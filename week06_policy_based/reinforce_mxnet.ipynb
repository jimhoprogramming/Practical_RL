{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE in pytorch\n",
    "\n",
    "Just like we did before for q-learning, this time we'll design a pytorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
    "\n",
    "Most of the code in this notebook is taken from approximate qlearning, so you'll find it more or less familiar and even simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # in google colab uncomment this\n",
    "\n",
    "# import os\n",
    "\n",
    "# os.system('apt-get install -y xvfb')\n",
    "# os.system('wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall18/xvfb -O ../xvfb')\n",
    "# os.system('apt-get install -y python-opengl ffmpeg')\n",
    "# os.system('pip install pyglet==1.2.4')\n",
    "\n",
    "# os.system('python -m pip install -U pygame --user')\n",
    "\n",
    "# print('setup complete')\n",
    "\n",
    "# XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY = : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry_point is :gym.envs.classic_control:CartPoleEnv\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jim/.local/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(require = False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f41c00ec438>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network for REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states. Let's define such a model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon.nn import HybridSequential as Sequential, Dense \n",
    "import mxnet\n",
    "from mxnet import nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple neural network that predicts policy logits. \n",
    "# Keep it simple: CartPole isn't worth deep architectures.\n",
    "# < YOUR CODE HERE: define a neural network that predicts policy logits >\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(2, activation=\"relu\"))\n",
    "model.add(Dense(3, activation=\"relu\"))\n",
    "model.add(Dense(2))\n",
    "# init\n",
    "model.initialize(init = mxnet.init.Xavier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probs(states):\n",
    "    \"\"\" \n",
    "    Predict action probabilities given states.\n",
    "    :param states: numpy array of shape [batch, state_shape]\n",
    "    :returns: numpy array of shape [batch, n_actions]\n",
    "    \"\"\"\n",
    "    # convert states, compute logits, use softmax to get probability\n",
    "    #<your code here >\n",
    "    states = nd.array(states)\n",
    "    logits = model(states)\n",
    "    policy = nd.softmax(logits)\n",
    "    #log_policy = nd.log_softmax(logits)\n",
    "    return policy.asnumpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5        0.5       ]\n",
      " [0.49948496 0.500515  ]\n",
      " [0.4996332  0.50036687]\n",
      " [0.5        0.5       ]\n",
      " [0.50199145 0.49800855]]\n"
     ]
    }
   ],
   "source": [
    "test_states = np.array([env.reset() for _ in range(5)])\n",
    "test_probas = predict_probs(test_states)\n",
    "print(test_probas)\n",
    "assert isinstance(\n",
    "    test_probas, np.ndarray), \"you must return np array and not %s\" % type(test_probas)\n",
    "assert tuple(test_probas.shape) == (\n",
    "    test_states.shape[0], env.action_space.n), \"wrong output shape: %s\" % np.shape(test_probas)\n",
    "assert np.allclose(np.sum(test_probas, axis=1),\n",
    "                   1), \"probabilities do not sum to 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play the game\n",
    "\n",
    "We can now use our newly built agent to play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \"\"\" \n",
    "    play a full session with REINFORCE agent and train at the session end.\n",
    "    returns sequences of states, actions andrewards\n",
    "    \"\"\"\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # action probabilities array aka pi(a|s)\n",
    "        action_probs = predict_probs(np.array([s]))[0]\n",
    "\n",
    "        # Sample action with given probabilities.\n",
    "        a = np.random.choice([0,1], size = 1, p = action_probs)[0]  # < your code >\n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        # record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.01998794, 0.0226609 , 0.01560493, 0.0126363 ]), array([ 0.02044116,  0.21755562,  0.01585765, -0.27508252]), array([ 0.02479227,  0.41244778,  0.010356  , -0.56272204]), array([ 0.03304122,  0.21718205, -0.00089844, -0.26679454]), array([ 0.03738486,  0.02207293, -0.00623433,  0.02560487]), array([ 0.03782632,  0.21728373, -0.00572223, -0.26903851]), array([ 0.042172  ,  0.0222439 , -0.011103  ,  0.02183411]), array([ 0.04261688, -0.17271707, -0.01066632,  0.31099333]), array([ 0.03916253, -0.36768545, -0.00444645,  0.60029342]), array([ 0.03180883, -0.17250158,  0.00755942,  0.30621325]), array([0.02835879, 0.02251184, 0.01368368, 0.01592393]), array([ 0.02880903, -0.17280365,  0.01400216,  0.31289262]), array([ 0.02535296, -0.36812225,  0.02026001,  0.60995829]), array([ 0.01799051, -0.56352145,  0.03245918,  0.90895299]), array([ 0.00672008, -0.75906736,  0.05063824,  1.21165867]), array([-0.00846126, -0.95480511,  0.07487141,  1.51977021]), array([-0.02755737, -0.76066384,  0.10526682,  1.25136579]), array([-0.04277064, -0.56703634,  0.13029413,  0.99342357]), array([-0.05411137, -0.76363789,  0.1501626 ,  1.32402441]), array([-0.06938413, -0.57069735,  0.17664309,  1.08185249]), array([-0.08079807, -0.37829044,  0.19828014,  0.84939656])] [1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0] [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# test it\n",
    "states, actions, rewards = generate_session()\n",
    "print(states,actions,rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cumulative rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards,  # rewards at each step\n",
    "                           gamma=0.99  # discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    take a list of immediate rewards r(s,a) for the whole session \n",
    "    compute cumulative returns (a.k.a. G(s,a) in Sutton '16)\n",
    "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "\n",
    "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
    "    and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
    "\n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    #<your code here >\n",
    "    rewards_list = []\n",
    "    m = len(rewards)\n",
    "    print('rewards len = {}'.format(m))\n",
    "    reverse_rewards = rewards.copy()\n",
    "    reverse_rewards.reverse()\n",
    "    for i in np.arange(m):\n",
    "        R_t = 0\n",
    "        p = 0\n",
    "        for r_t_1 in reverse_rewards:\n",
    "            power = (m - i - p - 1)\n",
    "            #print('r_t_1 = {}, power = {}'.format(r_t_1, power))\n",
    "            R_t += gamma ** power * r_t_1\n",
    "            p += 1\n",
    "            if (m - p) == i :\n",
    "                break\n",
    "        #print('add one = {}'.format(R_t))\n",
    "        rewards_list.append(R_t)\n",
    "    #print(rewards_list)\n",
    "    # < array of cumulative rewards >\n",
    "    return rewards_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards len = 21\n",
      "rewards len = 100\n",
      "rewards len = 7\n",
      "rewards len = 7\n",
      "rewards len = 7\n",
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "get_cumulative_rewards(rewards)\n",
    "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
    "assert np.allclose(get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9), [\n",
    "                   1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards(\n",
    "    [0, 0, 1, -2, 3, -4, 0], gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards(\n",
    "    [0, 0, 1, 2, 3, 4, 0], gamma=0), [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum  _{s_i,a_i} \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "\n",
    "Following the REINFORCE algorithm, we can define our objective as follows: \n",
    "\n",
    "$$ \\hat J \\approx { 1 \\over N } \\sum  _{s_i,a_i} log \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "<NDArray 3x10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "def to_one_hot(y_tensor, ndims):\n",
    "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
    "    '''\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    y_one_hot = torch.zeros(\n",
    "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
    "    '''\n",
    "    y_one_hot = nd.one_hot(indices = y_tensor, depth = ndims)\n",
    "    return y_one_hot\n",
    "print(to_one_hot(nd.array([1,2,3]),10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import Trainer\n",
    "\n",
    "# Your code: define optimizers\n",
    "# optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
    "\n",
    "\n",
    "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # cast everything into torch tensors\n",
    "    states = nd.array(states, dtype='float32')\n",
    "    actions = nd.array(actions, dtype='int32')\n",
    "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
    "    cumulative_returns = nd.array(cumulative_returns, dtype='float32')\n",
    "\n",
    "    # predict logits, probas and log-probas using an agent.\n",
    "    logits = model(states)\n",
    "    #print(logits)\n",
    "    probs = nd.softmax(logits)\n",
    "    log_probs = nd.log_softmax(logits)\n",
    "\n",
    "    assert all(isinstance(v, nd.NDArray) for v in [logits, probs, log_probs]), \\\n",
    "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
    "\n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    log_probs_for_actions = nd.sum(log_probs * to_one_hot(actions, env.action_space.n), \n",
    "                                   axis = 1)\n",
    "   \n",
    "    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n",
    "    entropy = mxnet.gluon.loss.SoftmaxCrossEntropyLoss() #< your code >\n",
    "    #loss =  entropy * nd.norm(cumulative_returns)   #< your code >\n",
    "    lr = 0.1\n",
    "    optimizer = 'adam'\n",
    "    optimizer_params={'learning_rate': lr}\n",
    "    all_weights = model.collect_params()\n",
    "    trainer = Trainer(params = all_weights, optimizer = optimizer, optimizer_params = optimizer_params)\n",
    "    # Gradient descent step\n",
    "    # < your code >\n",
    "    epochs = 1\n",
    "    batch_size = states.shape[0]\n",
    "    for epoch in range(epochs):\n",
    "        #tic = time.time()\n",
    "        #btic = time.time()\n",
    "        #accuracy.reset()\n",
    "        #\n",
    "        data = states\n",
    "        label = actions\n",
    "        with mxnet.autograd.record():\n",
    "            outputs = model(data)\n",
    "            entropy_out = entropy(outputs, label)\n",
    "            loss_out = nd.mean(entropy_out * outputs - nd.norm(cumulative_returns))\n",
    "        mxnet.autograd.backward(loss_out)\n",
    "        trainer.step(batch_size)\n",
    "        #accuracy.update([label], [outputs])\n",
    "   \n",
    "    # technical: return session rewards to print them later\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards len = 9\n"
     ]
    },
    {
     "ename": "MXNetError",
     "evalue": "Traceback (most recent call last):\n  File \"src/operator/numpy/linalg/./../../tensor/elemwise_binary_broadcast_op.h\", line 68\nMXNetError: Check failed: l == 1 || r == 1: operands could not be broadcast together with shapes [9] [9,2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMXNetError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-3d2fff506229>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     rewards = [train_on_session(*generate_session())\n\u001b[0;32m----> 3\u001b[0;31m                for _ in range(100)]  # generate new sessions\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mean reward:%.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-3d2fff506229>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     rewards = [train_on_session(*generate_session())\n\u001b[0;32m----> 3\u001b[0;31m                for _ in range(100)]  # generate new sessions\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mean reward:%.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-70970cad3d4a>\u001b[0m in \u001b[0;36mtrain_on_session\u001b[0;34m(states, actions, rewards, gamma, entropy_coef)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mentropy_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mloss_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentropy_out\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcumulative_returns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mmxnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__mul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;34m\"\"\"x.__mul__(y) <=> x*y <=> mx.nd.multiply(x, y) \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__neg__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36mmultiply\u001b[0;34m(lhs, rhs)\u001b[0m\n\u001b[1;32m   3756\u001b[0m         \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3757\u001b[0m         \u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_scalar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3758\u001b[0;31m         None)\n\u001b[0m\u001b[1;32m   3759\u001b[0m     \u001b[0;31m# pylint: enable= no-member, protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36m_ufunc_helper\u001b[0;34m(lhs, rhs, fn_array, fn_scalar, lfn_scalar, rfn_scalar)\u001b[0m\n\u001b[1;32m   3569\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlfn_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3570\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNDArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3571\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3572\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3573\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'type %s not supported'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/mxnet/ndarray/register.py\u001b[0m in \u001b[0;36mbroadcast_mul\u001b[0;34m(lhs, rhs, out, name, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/mxnet/_ctypes/ndarray.py\u001b[0m in \u001b[0;36m_imperative_invoke\u001b[0;34m(handle, ndargs, keys, vals, out, is_np_op, output_is_list)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mc_str_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mc_str_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         ctypes.byref(out_stypes)))\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mcreate_ndarray_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_global_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_np_ndarray_cls\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_np_op\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_global_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ndarray_cls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/mxnet/base.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \"\"\"\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mget_last_ffi_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMXNetError\u001b[0m: Traceback (most recent call last):\n  File \"src/operator/numpy/linalg/./../../tensor/elemwise_binary_broadcast_op.h\", line 68\nMXNetError: Check failed: l == 1 || r == 1: operands could not be broadcast together with shapes [9] [9,2]"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    rewards = [train_on_session(*generate_session())\n",
    "               for _ in range(100)]  # generate new sessions\n",
    "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
    "    if np.mean(rewards) > 500:\n",
    "        print(\"You Win!\")  # but you can train even further\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),\n",
    "                           directory=\"videos\", force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1]))  # this may or may not be the _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
