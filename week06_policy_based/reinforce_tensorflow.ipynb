{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE in TensorFlow (3 pts)¶\n",
    "\n",
    "This notebook implements a basic reinforce algorithm a.k.a. policy gradient for CartPole env.\n",
    "\n",
    "It has been deliberately written to be as simple and human-readable.\n",
    "\n",
    "Authors: [Practical_RL](https://github.com/yandexdataschool/Practical_RL) course team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='floatX=float32'\n",
      "needs-to-be-defined\n",
      "19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# 运行一个终段程序虚拟显示器0给它\\nif os.environ.get(\\'DISPLAY\\') == \\':0\\':\\n    print(\\'create display 0\\')\\n    !bash xvfb-run -a -s \"-screen 0 800x600x32\" bash\\n    %env DISPLAY = :0\\n\\n#if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\\n    #!bash xvfb-run -a bash\\n    !bash xvfb-run -s \"-screen 0 800x600x32\"\\n    #!bash ../xvfb start\\n    %env DISPLAY = : 0\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "要使用rander 就要一开始器启动jupyter时建立一个虚拟显示器通信给jupyter用\n",
    "命令为 xvfb -s \"-screen 0 1400x900x24\" jupyter-notebook\n",
    "'''\n",
    "%env THEANO_FLAGS = 'floatX=float32'\n",
    "import os\n",
    "print(os.environ.get('DISPLAY'))  # :1 str\n",
    "print(len(os.environ.get(\"DISPLAY\")))  # 2\n",
    "'''\n",
    "# 运行一个终段程序虚拟显示器0给它\n",
    "if os.environ.get('DISPLAY') == ':0':\n",
    "    print('create display 0')\n",
    "    !bash xvfb-run -a -s \"-screen 0 800x600x32\" bash\n",
    "    %env DISPLAY = :0\n",
    "\n",
    "#if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    #!bash xvfb-run -a bash\n",
    "    !bash xvfb-run -s \"-screen 0 800x600x32\"\n",
    "    #!bash ../xvfb start\n",
    "    %env DISPLAY = : 0\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook assumes that you have [openai gym](https://github.com/openai/gym) installed.\n",
    "\n",
    "In case you're running on a server, [use xvfb](https://github.com/openai/gym#rendering-on-a-server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "[ 0.04293506 -0.0006653  -0.01832352 -0.02778047]\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xa3b3148>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# gym compatibility: unwrap TimeLimit\n",
    "if hasattr(env, 'env'):\n",
    "    env = env.env\n",
    "\n",
    "s = env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "print(state_dim)\n",
    "print(s)\n",
    "print(n_actions)\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network for REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04293506 -0.0006653  -0.01832352 -0.02778047]]\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "'''\n",
    "# 使用mxnet替代tf\n",
    "import mxnet\n",
    "\n",
    "# create input variables. We only need <s,a,R> for REINFORCE\n",
    "'''\n",
    "states = tf.placeholder('float32', (None,)+state_dim, name=\"states\")\n",
    "actions = tf.placeholder('int32', name=\"action_ids\")\n",
    "cumulative_rewards = tf.placeholder('float32', name=\"cumulative_returns\")\n",
    "'''\n",
    "states = s.reshape(1,-1)   # an array of 4 numbers\n",
    "print(states)\n",
    "actions = [0,1]  #  0 or 1\n",
    "print(actions)\n",
    "cumulative = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "<define network graph using raw tf or any deep learning library >\n",
    "'''\n",
    "from mxnet.gluon.nn import HybridSequential as Sequential, Dense \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(2, activation=\"relu\"))\n",
    "model.add(Dense(3, activation=\"relu\"))\n",
    "model.add(Dense(n_actions))\n",
    "# init\n",
    "model.initialize(init = mxnet.init.Xavier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[0.5 0.5]]\n",
      "<NDArray 1x2 @cpu(0)>\n",
      "\n",
      "[[-0.6931472 -0.6931472]]\n",
      "<NDArray 1x2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "# utility function to pick action in one given state\n",
    "from mxnet import nd\n",
    "\n",
    "def get_action_proba(s):\n",
    "    logits = model(nd.array(s))\n",
    "    policy = nd.softmax(logits)\n",
    "    log_policy = nd.log_softmax(logits)\n",
    "    return policy, log_policy\n",
    "policy, log_policy = get_action_proba(states)\n",
    "print(policy)\n",
    "print(log_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum  _{s_i,a_i} \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "\n",
    "Following the REINFORCE algorithm, we can define our objective as follows: \n",
    "\n",
    "$$ \\hat J \\approx { 1 \\over N } \\sum  _{s_i,a_i} log \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': -0.6931471805599453, '1': -0.6931471805599453}\n"
     ]
    }
   ],
   "source": [
    "# get probabilities for parti \n",
    "# 搞甘复杂的一个按索引插入张量表不如用dict,真不明白这老师的想法。\n",
    "#tf.stack?\n",
    "#tf.gather_nd?\n",
    "'''\n",
    "indices = tf.stack([tf.range(actions.shape[-1]), actions], axis=-1)\n",
    "log_policy_for_actions = tf.gather_nd(log_policy, indices)\n",
    "'''\n",
    "log_policy_for_actions = {str(a):l_p for a,l_p in zip(actions, log_policy)}\n",
    "print(log_policy_for_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-49-81e85012bcca>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-49-81e85012bcca>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    J =  # <policy objective as in the last formula. Please use mean, not sum.>\u001b[0m\n\u001b[0m                                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# REINFORCE objective function\n",
    "# hint: you need to use log_policy_for_actions to get log probabilities for actions taken\n",
    "\n",
    "# <policy objective as in the last formula. Please use mean, not sum.>\n",
    "J = tf.mean(tf.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# regularize with entropy\n",
    "entropy = <compute entropy. Don't forget the sign!>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all network weights\n",
    "all_weights = <a list of all trainable weights in your network >\n",
    "\n",
    "# weight updates. maximizing J is same as minimizing -J. Adding negative entropy.\n",
    "loss = -J - 0.1*entropy\n",
    "\n",
    "update = tf.train.AdamOptimizer().minimize(loss, var_list=all_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cumulative rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_cumulative_rewards(rewards,  # rewards at each step\n",
    "                           gamma=0.99  # discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    take a list of immediate rewards r(s,a) for the whole session \n",
    "    compute cumulative rewards R(s,a) (a.k.a. G(s,a) in Sutton '16)\n",
    "    R_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "\n",
    "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
    "    and compute R_t = r_t + gamma*R_{t+1} recurrently\n",
    "\n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    #<your code here >\n",
    "    rewards_list = []\n",
    "    m = len(rewards)\n",
    "    print('rewards len = {}'.format(m))\n",
    "    reverse_rewards = rewards.copy()\n",
    "    reverse_rewards.reverse()\n",
    "    for i in np.arange(m):\n",
    "        R_t = 0\n",
    "        p = 0\n",
    "        for r_t_1 in reverse_rewards:\n",
    "            power = (m - i - p - 1)\n",
    "            #print('r_t_1 = {}, power = {}'.format(r_t_1, power))\n",
    "            R_t += gamma ** power * r_t_1\n",
    "            p += 1\n",
    "            if (m - p) == i :\n",
    "                break\n",
    "        #print('add one = {}'.format(R_t))\n",
    "        rewards_list.append(R_t)\n",
    "    #print(rewards_list)\n",
    "    return rewards_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards len = 100\n",
      "rewards len = 7\n",
      "rewards len = 7\n",
      "rewards len = 7\n",
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "#get_cumulative_rewards([i for i in range(10)])\n",
    "\n",
    "assert len(get_cumulative_rewards([i for i in range(100)])) == 100\n",
    "assert np.allclose(get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9), [\n",
    "                   1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards(\n",
    "    [0, 0, 1, -2, 3, -4, 0], gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards(\n",
    "    [0, 0, 1, 2, 3, 4, 0], gamma=0), [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(_states, _actions, _rewards):\n",
    "    \"\"\"given full session, trains agent with policy gradient\"\"\"\n",
    "    _cumulative_rewards = get_cumulative_rewards(_rewards)\n",
    "    update.run({states: _states, actions: _actions,\n",
    "                cumulative_rewards: _cumulative_rewards})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \"\"\"play env with REINFORCE agent and train at the session end\"\"\"\n",
    "\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "\n",
    "        # action probabilities array aka pi(a|s)\n",
    "        action_probas = get_action_proba(s)\n",
    "        # <pick random action using action_probas >\n",
    "        a = np.random.choice(action, action_probas)\n",
    "\n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        # record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    train_step(states, actions, rewards)\n",
    "\n",
    "    return sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:27.590\n",
      "mean reward:70.340\n",
      "mean reward:129.570\n",
      "mean reward:188.330\n",
      "mean reward:211.530\n",
      "mean reward:240.490\n",
      "mean reward:235.760\n",
      "mean reward:218.030\n",
      "mean reward:258.470\n",
      "mean reward:184.760\n",
      "mean reward:298.920\n",
      "mean reward:507.360\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "s = tf.InteractiveSession()\n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    rewards = [generate_session() for _ in range(100)]  # generate new sessions\n",
    "\n",
    "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
    "\n",
    "    if np.mean(rewards) > 300:\n",
    "        print(\"You Win!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results & video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-08 03:29:10,315] Making new env: CartPole-v0\n",
      "[2017-04-08 03:29:10,324] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-04-08 03:29:10,329] Clearing 6 monitor files from previous run (because force=True was provided)\n",
      "[2017-04-08 03:29:10,336] Starting new video recorder writing to /home/jheuristic/Downloads/sonnet/sonnet/examples/videos/openaigym.video.0.14221.video000000.mp4\n",
      "[2017-04-08 03:29:16,834] Starting new video recorder writing to /home/jheuristic/Downloads/sonnet/sonnet/examples/videos/openaigym.video.0.14221.video000001.mp4\n",
      "[2017-04-08 03:29:23,689] Starting new video recorder writing to /home/jheuristic/Downloads/sonnet/sonnet/examples/videos/openaigym.video.0.14221.video000008.mp4\n",
      "[2017-04-08 03:29:33,407] Starting new video recorder writing to /home/jheuristic/Downloads/sonnet/sonnet/examples/videos/openaigym.video.0.14221.video000027.mp4\n",
      "[2017-04-08 03:29:45,840] Starting new video recorder writing to /home/jheuristic/Downloads/sonnet/sonnet/examples/videos/openaigym.video.0.14221.video000064.mp4\n",
      "[2017-04-08 03:29:56,812] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/jheuristic/Downloads/sonnet/sonnet/examples/videos')\n"
     ]
    }
   ],
   "source": [
    "# record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),\n",
    "                           directory=\"videos\", force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./videos/openaigym.video.0.14221.video000027.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# That's all, thank you for your attention!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
