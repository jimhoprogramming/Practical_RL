{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE in TensorFlow (3 pts)¶\n",
    "\n",
    "This notebook implements a basic reinforce algorithm a.k.a. policy gradient for CartPole env.\n",
    "\n",
    "It has been deliberately written to be as simple and human-readable.\n",
    "\n",
    "Authors: [Practical_RL](https://github.com/yandexdataschool/Practical_RL) course team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='floatX=float32'\n",
      ":99\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# 运行一个终段程序虚拟显示器0给它\\nif os.environ.get(\\'DISPLAY\\') == \\':0\\':\\n    print(\\'create display 0\\')\\n    !bash xvfb-run -a -s \"-screen 0 800x600x32\" bash\\n    %env DISPLAY = :0\\n\\n#if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\\n    #!bash xvfb-run -a bash\\n    !bash xvfb-run -s \"-screen 0 800x600x32\"\\n    #!bash ../xvfb start\\n    %env DISPLAY = : 0\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "要使用rander 就要一开始器启动jupyter时建立一个虚拟显示器通信给jupyter用\n",
    "命令为 xvfb -s \"-screen 0 1400x900x24\" jupyter-notebook\n",
    "'''\n",
    "%env THEANO_FLAGS = 'floatX=float32'\n",
    "import os\n",
    "print(os.environ.get('DISPLAY'))  # :1 str\n",
    "print(len(os.environ.get(\"DISPLAY\")))  # 2\n",
    "'''\n",
    "# 运行一个终段程序虚拟显示器0给它\n",
    "if os.environ.get('DISPLAY') == ':0':\n",
    "    print('create display 0')\n",
    "    !bash xvfb-run -a -s \"-screen 0 800x600x32\" bash\n",
    "    %env DISPLAY = :0\n",
    "\n",
    "#if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    #!bash xvfb-run -a bash\n",
    "    !bash xvfb-run -s \"-screen 0 800x600x32\"\n",
    "    #!bash ../xvfb start\n",
    "    %env DISPLAY = : 0\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook assumes that you have [openai gym](https://github.com/openai/gym) installed.\n",
    "\n",
    "In case you're running on a server, [use xvfb](https://github.com/openai/gym#rendering-on-a-server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry_point is :gym.envs.classic_control:CartPoleEnv\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "(4,)\n",
      "[ 0.00597587  0.02153169 -0.02694756 -0.0181432 ]\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jim/.local/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(require = False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1d2804fc50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAEldJREFUeJzt3X+MnVd95/H3p3ZIKLB1QmYtyz/WabEWpVVxstPgCLRKE9E62apOpRYlW5UIRZpUMhKoaLdJV9qC1EjtHyW7aNuobpNiKpaQBmisKC1NTaSKP0iwwRg7JmUAR7blxA4kARZtdh2+/WOO4eKMPXfmzvV4Du+XdHWf5zznPvd7kqvPPHPmOb6pKiRJ/fmppS5AkjQeBrwkdcqAl6ROGfCS1CkDXpI6ZcBLUqfGFvBJtiZ5Osl0kjvH9T6SpNllHPfBJ1kB/AvwDuAo8AXg1qp6atHfTJI0q3FdwV8DTFfVN6rq/wEPANvG9F6SpFmsHNN51wJHBvaPAm89W+fLL7+8Nm7cOKZSJGn5OXz4MM8//3xGOce4An5OSaaAKYANGzawZ8+epSpFki44k5OTI59jXFM0x4D1A/vrWtsPVdWOqpqsqsmJiYkxlSFJP7nGFfBfADYluSLJa4BbgF1jei9J0izGMkVTVaeSvAf4DLACuL+qDo7jvSRJsxvbHHxVPQo8Oq7zS5LOzZWsktQpA16SOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6ROGfCS1CkDXpI6NdJX9iU5DHwXeAU4VVWTSS4DPgFsBA4D76yqF0YrU5I0X4txBf/LVbW5qibb/p3A7qraBOxu+5Kk82wcUzTbgJ1teydw8xjeQ5I0h1EDvoB/TLI3yVRrW11Vx9v2s8DqEd9DkrQAI83BA2+vqmNJ/i3wWJKvDh6sqkpSs72w/UCYAtiwYcOIZUiSzjTSFXxVHWvPJ4BPA9cAzyVZA9CeT5zltTuqarKqJicmJkYpQ5I0iwUHfJLXJXnD6W3gV4ADwC7gttbtNuDhUYuUJM3fKFM0q4FPJzl9nv9dVf+Q5AvAg0luB54B3jl6mZKk+VpwwFfVN4C3zNL+LeCGUYqSJI3OlayS1CkDXpI6ZcBLUqcMeEnqlAEvSZ0y4CWpUwa8JHXKgJekThnwktQpA16SOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSp+YM+CT3JzmR5MBA22VJHkvytfZ8aWtPkg8nmU6yP8nV4yxeknR2w1zBfwTYekbbncDuqtoE7G77ADcCm9pjCrh3ccqUJM3XnAFfVf8MfPuM5m3Azra9E7h5oP2jNePzwKokaxarWEnS8BY6B7+6qo637WeB1W17LXBkoN/R1vYqSaaS7Emy5+TJkwssQ5J0NiP/kbWqCqgFvG5HVU1W1eTExMSoZUiSzrDQgH/u9NRLez7R2o8B6wf6rWttkqTzbKEBvwu4rW3fBjw80P6udjfNFuClgakcSdJ5tHKuDkk+DlwHXJ7kKPCHwB8DDya5HXgGeGfr/ihwEzANfB949xhqliQNYc6Ar6pbz3Lohln6FrB91KIkSaNzJaskdcqAl6ROGfCS1CkDXpI6ZcBLUqcMeEnqlAEvSZ0y4CWpUwa8JHXKgJekThnwktQpA16SOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE7NGfBJ7k9yIsmBgbYPJDmWZF973DRw7K4k00meTvKr4ypcknRuw1zBfwTYOkv7PVW1uT0eBUhyJXAL8PPtNX+eZMViFStJGt6cAV9V/wx8e8jzbQMeqKqXq+qbwDRwzQj1SZIWaJQ5+Pck2d+mcC5tbWuBIwN9jra2V0kylWRPkj0nT54coQxJ0mwWGvD3Aj8HbAaOA3863xNU1Y6qmqyqyYmJiQWWIUk6mwUFfFU9V1WvVNUPgL/kR9Mwx4D1A13XtTZJ0nm2oIBPsmZg9zeA03fY7AJuSXJxkiuATcCTo5UoSVqIlXN1SPJx4Drg8iRHgT8ErkuyGSjgMHAHQFUdTPIg8BRwCtheVa+Mp3RJ0rnMGfBVdesszfedo//dwN2jFCVJGp0rWSWpUwa8JHXKgJekThnwktQpA16SOmXAS1Kn5rxNUurd3h13vKrtP0z9xRJUIi0ur+AlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6ROzRnwSdYneTzJU0kOJnlva78syWNJvtaeL23tSfLhJNNJ9ie5etyDkCS92jBX8KeA91fVlcAWYHuSK4E7gd1VtQnY3fYBbgQ2tccUcO+iVy1JmtOcAV9Vx6vqi237u8AhYC2wDdjZuu0Ebm7b24CP1ozPA6uSrFn0yiVJ5zSvOfgkG4GrgCeA1VV1vB16FljdttcCRwZedrS1nXmuqSR7kuw5efLkPMuWJM1l6IBP8nrgk8D7quo7g8eqqoCazxtX1Y6qmqyqyYmJifm8VJI0hKECPslFzIT7x6rqU635udNTL+35RGs/BqwfePm61iZJOo+GuYsmwH3Aoar60MChXcBtbfs24OGB9ne1u2m2AC8NTOVIks6TYb6y723A7wBfSbKvtf0B8MfAg0luB54B3tmOPQrcBEwD3wfevagVS5KGMmfAV9XngJzl8A2z9C9g+4h1SUtq7447/F5WLXuuZJWkThnwktQpA16SOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6ROGfCS1KlhvnR7fZLHkzyV5GCS97b2DyQ5lmRfe9w08Jq7kkwneTrJr45zAJKk2Q1zBX8KeH9VXQlsAbYnubIdu6eqNrfHowDt2C3AzwNbgT9PsmIMtUuLwu9eVa/mDPiqOl5VX2zb3wUOAWvP8ZJtwANV9XJVfROYBq5ZjGIlScOb1xx8ko3AVcATrek9SfYnuT/Jpa1tLXBk4GVHOfcPBEnSGAwd8EleD3wSeF9VfQe4F/g5YDNwHPjT+bxxkqkke5LsOXny5HxeKkkawlABn+QiZsL9Y1X1KYCqeq6qXqmqHwB/yY+mYY4B6wdevq61/Ziq2lFVk1U1OTExMcoYJEmzGOYumgD3AYeq6kMD7WsGuv0GcKBt7wJuSXJxkiuATcCTi1eyJGkYK4fo8zbgd4CvJNnX2v4AuDXJZqCAw8AdAFV1MMmDwFPM3IGzvapeWezCJUnnNmfAV9XngMxy6NFzvOZu4O4R6pIkjciVrJLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6ROGfCS1CkDXpI6ZcBLUqcMeEnqlAEvncXeHXcsdQnSSAx4dSvJ0I9xnkNaKga8JHVqmC/8kH4iPHJ86sf2f23NjiWqRFocXsFLvDrcpR4Y8NJZGPpa7ob50u1LkjyZ5MtJDib5YGu/IskTSaaTfCLJa1r7xW1/uh3fON4hSOPhFI2Wu2Gu4F8Grq+qtwCbga1JtgB/AtxTVW8CXgBub/1vB15o7fe0ftIFzTBXj4b50u0Cvtd2L2qPAq4H/nNr3wl8ALgX2Na2AR4C/leStPNIF6TJO3YAPx7yH1iSSqTFM9RdNElWAHuBNwF/BnwdeLGqTrUuR4G1bXstcASgqk4leQl4I/D82c6/d+9e7yPWsudnWBeaoQK+ql4BNidZBXwaePOob5xkCpgC2LBhA88888yop5R+zPkOXH9J1WKanJwc+Rzzuoumql4EHgeuBVYlOf0DYh1wrG0fA9YDtOM/A3xrlnPtqKrJqpqcmJhYYPmSpLMZ5i6aiXblTpLXAu8ADjET9L/Zut0GPNy2d7V92vHPOv8uSeffMFM0a4CdbR7+p4AHq+qRJE8BDyT5I+BLwH2t/33A3ySZBr4N3DKGuiVJcxjmLpr9wFWztH8DuGaW9v8L/NaiVCdJWjBXskpSpwx4SeqUAS9JnfKfC1a3vHlLP+m8gpekThnwktQpA16SOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnRrmS7cvSfJkki8nOZjkg639I0m+mWRfe2xu7Uny4STTSfYnuXrcg5Akvdow/x78y8D1VfW9JBcBn0vy9+3Yf6mqh87ofyOwqT3eCtzbniVJ59GcV/A143tt96L2ONc3KWwDPtpe93lgVZI1o5cqSZqPoebgk6xIsg84ATxWVU+0Q3e3aZh7klzc2tYCRwZefrS1SZLOo6ECvqpeqarNwDrgmiS/ANwFvBn4JeAy4Pfn88ZJppLsSbLn5MmT8yxbkjSXed1FU1UvAo8DW6vqeJuGeRn4a+Ca1u0YsH7gZeta25nn2lFVk1U1OTExsbDqJUlnNcxdNBNJVrXt1wLvAL56el49SYCbgQPtJbuAd7W7abYAL1XV8bFUL0k6q2HuolkD7EyygpkfCA9W1SNJPptkAgiwD/jd1v9R4CZgGvg+8O7FL1uSNJc5A76q9gNXzdJ+/Vn6F7B99NIkSaNwJaskdcqAl6ROGfCS1CkDXpI6ZcBLUqcMeEnqlAEvSZ0y4CWpUwa8JHXKgJekThnwktQpA16SOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUqaEDPsmKJF9K8kjbvyLJE0mmk3wiyWta+8Vtf7od3zie0iVJ5zKfK/j3AocG9v8EuKeq3gS8ANze2m8HXmjt97R+kqTzbKiAT7IO+E/AX7X9ANcDD7UuO4Gb2/a2tk87fkPrL0k6j1YO2e9/AP8VeEPbfyPwYlWdavtHgbVtey1wBKCqTiV5qfV/fvCESaaAqbb7cpIDCxrBhe9yzhh7J3odF/Q7Nse1vPy7JFNVtWOhJ5gz4JP8GnCiqvYmuW6hb3SmVvSO9h57qmpysc59Iel1bL2OC/odm+NafpLsoeXkQgxzBf824NeT3ARcAvwb4H8Cq5KsbFfx64Bjrf8xYD1wNMlK4GeAby20QEnSwsw5B19Vd1XVuqraCNwCfLaqfht4HPjN1u024OG2vavt045/tqpqUauWJM1plPvgfx/4vSTTzMyx39fa7wPe2Np/D7hziHMt+FeQZaDXsfU6Luh3bI5r+RlpbPHiWpL65EpWSerUkgd8kq1Jnm4rX4eZzrmgJLk/yYnB2zyTXJbksSRfa8+XtvYk+XAb6/4kVy9d5eeWZH2Sx5M8leRgkve29mU9tiSXJHkyyZfbuD7Y2rtYmd3rivMkh5N8Jcm+dmfJsv8sAiRZleShJF9NcijJtYs5riUN+CQrgD8DbgSuBG5NcuVS1rQAHwG2ntF2J7C7qjYBu/nR3yFuBDa1xxRw73mqcSFOAe+vqiuBLcD29v9muY/tZeD6qnoLsBnYmmQL/azM7nnF+S9X1eaBWyKX+2cRZu5I/IeqejPwFmb+3y3euKpqyR7AtcBnBvbvAu5aypoWOI6NwIGB/aeBNW17DfB02/4L4NbZ+l3oD2buknpHT2MDfhr4IvBWZhbKrGztP/xcAp8Brm3bK1u/LHXtZxnPuhYI1wOPAOlhXK3Gw8DlZ7Qt688iM7eQf/PM/+6LOa6lnqL54arXZnBF7HK2uqqOt+1ngdVte1mOt/36fhXwBB2MrU1j7ANOAI8BX2fIldnA6ZXZF6LTK85/0PaHXnHOhT0ugAL+Mcnetgoelv9n8QrgJPDXbVrtr5K8jkUc11IHfPdq5kftsr1VKcnrgU8C76uq7wweW65jq6pXqmozM1e81wBvXuKSRpaBFedLXcuYvL2qrmZmmmJ7kv84eHCZfhZXAlcD91bVVcD/4Yzbykcd11IH/OlVr6cNrohdzp5LsgagPZ9o7ctqvEkuYibcP1ZVn2rNXYwNoKpeZGbB3rW0ldnt0Gwrs7nAV2afXnF+GHiAmWmaH644b32W47gAqKpj7fkE8GlmfjAv98/iUeBoVT3R9h9iJvAXbVxLHfBfADa1v/S/hpmVsruWuKbFMLia98xVvu9qfw3fArw08KvYBSVJmFm0dqiqPjRwaFmPLclEklVt+7XM/F3hEMt8ZXZ1vOI8yeuSvOH0NvArwAGW+Wexqp4FjiT5963pBuApFnNcF8AfGm4C/oWZedD/ttT1LKD+jwPHgf/PzE/k25mZy9wNfA34J+Cy1jfM3DX0deArwORS13+Ocb2dmV8N9wP72uOm5T424BeBL7VxHQD+e2v/WeBJYBr4W+Di1n5J259ux392qccwxBivAx7pZVxtDF9uj4Onc2K5fxZbrZuBPe3z+HfApYs5LleySlKnlnqKRpI0Jga8JHXKgJekThnwktQpA16SOmXAS1KnDHhJ6pQBL0md+lev2oDHeF7c0wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# gym compatibility: unwrap TimeLimit\n",
    "if hasattr(env, 'env'):\n",
    "    env = env.env\n",
    "\n",
    "s = env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "print(state_dim)\n",
    "print(s)\n",
    "print(n_actions)\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network for REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00597587  0.02153169 -0.02694756 -0.0181432 ]]\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "# create input variables. We only need <s,a,R> for REINFORCE\n",
    "'''\n",
    "states = tf.placeholder('float32', (None,)+state_dim, name=\"states\")\n",
    "actions = tf.placeholder('int32', name=\"action_ids\")\n",
    "cumulative_rewards = tf.placeholder('float32', name=\"cumulative_returns\")\n",
    "'''\n",
    "states = s.reshape(1,-1)   # an array of 4 numbers\n",
    "print(states)\n",
    "actions = [0,1]  #  0 or 1\n",
    "print(actions)\n",
    "cumulative = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "<define network graph using raw tf or any deep learning library >\n",
    "'''\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(2, activation=\"relu\"))\n",
    "model.add(layers.Dense(3, activation=\"relu\"))\n",
    "model.add(layers.Dense(n_actions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.5]\n",
      "[-0.6931471805599453, -0.6931471805599453]\n"
     ]
    }
   ],
   "source": [
    "# utility function to pick action in one given state\n",
    "def get_action_proba(s):\n",
    "    logits = model(s)\n",
    "    policy = tf.nn.softmax(logits)\n",
    "    log_policy = tf.nn.log_softmax(logits)\n",
    "    return policy.numpy().tolist()[0], log_policy.numpy().tolist()[0]\n",
    "policy, log_policy = get_action_proba(states)\n",
    "print(policy)\n",
    "print(log_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum  _{s_i,a_i} \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "\n",
    "Following the REINFORCE algorithm, we can define our objective as follows: \n",
    "\n",
    "$$ \\hat J \\approx { 1 \\over N } \\sum  _{s_i,a_i} log \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': -0.6931471805599453, '1': -0.6931471805599453}\n"
     ]
    }
   ],
   "source": [
    "# get probabilities for parti \n",
    "# 搞甘复杂的一个按索引插入张量表不如用dict,真不明白这老师的想法。\n",
    "#tf.stack?\n",
    "#tf.gather_nd?\n",
    "'''\n",
    "indices = tf.stack([tf.range(actions.shape[-1]), actions], axis=-1)\n",
    "log_policy_for_actions = tf.gather_nd(log_policy, indices)\n",
    "'''\n",
    "log_policy_for_actions = {str(a):l_p for a,l_p in zip(actions, log_policy)}\n",
    "print(log_policy_for_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-49-81e85012bcca>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-49-81e85012bcca>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    J =  # <policy objective as in the last formula. Please use mean, not sum.>\u001b[0m\n\u001b[0m                                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# REINFORCE objective function\n",
    "# hint: you need to use log_policy_for_actions to get log probabilities for actions taken\n",
    "\n",
    "# <policy objective as in the last formula. Please use mean, not sum.>\n",
    "J = tf.mean(tf.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# regularize with entropy\n",
    "entropy = <compute entropy. Don't forget the sign!>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all network weights\n",
    "all_weights = <a list of all trainable weights in your network >\n",
    "\n",
    "# weight updates. maximizing J is same as minimizing -J. Adding negative entropy.\n",
    "loss = -J - 0.1*entropy\n",
    "\n",
    "update = tf.train.AdamOptimizer().minimize(loss, var_list=all_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cumulative rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_cumulative_rewards(rewards,  # rewards at each step\n",
    "                           gamma=0.99  # discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    take a list of immediate rewards r(s,a) for the whole session \n",
    "    compute cumulative rewards R(s,a) (a.k.a. G(s,a) in Sutton '16)\n",
    "    R_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "\n",
    "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
    "    and compute R_t = r_t + gamma*R_{t+1} recurrently\n",
    "\n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    #<your code here >\n",
    "    rewards_list = []\n",
    "    m = len(rewards)\n",
    "    print('rewards len = {}'.format(m))\n",
    "    for r_t in rewards.reverse():\n",
    "        \n",
    "        print(R_t)\n",
    "        rewards_list.append(R_t)\n",
    "    return rewards_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2642.380210770436\n",
      "2669.060818960035\n",
      "2694.990726222259\n",
      "2720.1623497194537\n",
      "2744.5680300196505\n",
      "2768.2000303228783\n",
      "2791.050535679675\n",
      "2813.1116522016914\n",
      "2834.3754062643357\n",
      "2854.83374370135\n",
      "2874.4785289912616\n",
      "2893.301544435619\n",
      "2911.2944893289077\n",
      "2928.448979120107\n",
      "2944.7565445657665\n",
      "2960.2086308745115\n",
      "2974.7965968429403\n",
      "2988.511713982769\n",
      "3001.34516563916\n",
      "3013.2880461001623\n",
      "3024.331359697133\n",
      "3034.466019896094\n",
      "3043.6828483798927\n",
      "3051.972574121104\n",
      "3059.32583244556\n",
      "3065.7331640864236\n",
      "3071.1850142287117\n",
      "3075.671731544152\n",
      "3079.183567216315\n",
      "3081.710673955874\n",
      "3083.2431050059336\n",
      "3083.7708131373074\n",
      "3083.283649633643\n",
      "3081.771363266306\n",
      "3079.223599258896\n",
      "3075.629898241307\n",
      "3070.9796951932394\n",
      "3065.262318377011\n",
      "3058.4669882596068\n",
      "3050.582816423846\n",
      "3041.59880446853\n",
      "3031.5038428975063\n",
      "3020.2867099974806\n",
      "3007.936070704527\n",
      "2994.4404754591174\n",
      "2979.788359049614\n",
      "2963.968039444055\n",
      "2946.967716610157\n",
      "2928.7754713233894\n",
      "2909.379263963019\n",
      "2888.766933295979\n",
      "2866.9261952484644\n",
      "2843.8446416651154\n",
      "2819.509739055672\n",
      "2793.908827328961\n",
      "2767.0291185141036\n",
      "2738.857695468791\n",
      "2709.381510574536\n",
      "2678.5873844187236\n",
      "2646.462004463357\n",
      "2612.991923700361\n",
      "2578.1635592932935\n",
      "2541.9631912053474\n",
      "2504.3769608134826\n",
      "2465.390869508568\n",
      "2424.9907772813817\n",
      "2383.1624012943253\n",
      "2339.8913144387125\n",
      "2295.162943877487\n",
      "2248.9625695732198\n",
      "2201.2753228012316\n",
      "2152.086184647709\n",
      "2101.3799844926352\n",
      "2049.14139847741\n",
      "1995.354947956979\n",
      "1940.0049979363425\n",
      "1883.075755491255\n",
      "1824.5512681729847\n",
      "1764.4154223969545\n",
      "1702.6519418151056\n",
      "1639.2443856718235\n",
      "1574.1761471432562\n",
      "1507.4304516598547\n",
      "1438.990355211975\n",
      "1368.838742638358\n",
      "1296.9583258973316\n",
      "1223.3316423205367\n",
      "1147.9410528490273\n",
      "1070.7687402515426\n",
      "991.7967073247906\n",
      "911.006775075546\n",
      "828.3805808843899\n",
      "743.8995766508991\n",
      "657.5450269201\n",
      "569.29800699\n",
      "479.13940099999996\n",
      "387.0499\n",
      "293.01\n",
      "197.0\n",
      "99\n",
      "1.5561\n",
      "1.729\n",
      "1.81\n",
      "0.9\n",
      "1.0\n",
      "1.0\n",
      "0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-c97c71f54a0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_cumulative_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m assert np.allclose(get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9), [\n\u001b[0;32m----> 3\u001b[0;31m                    1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n\u001b[0m\u001b[1;32m      4\u001b[0m assert np.allclose(get_cumulative_rewards(\n\u001b[1;32m      5\u001b[0m     [0, 0, 1, -2, 3, -4, 0], gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert len(get_cumulative_rewards(range(100))) == 100\n",
    "assert np.allclose(get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9), [\n",
    "                   1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards(\n",
    "    [0, 0, 1, -2, 3, -4, 0], gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards(\n",
    "    [0, 0, 1, 2, 3, 4, 0], gamma=0), [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(_states, _actions, _rewards):\n",
    "    \"\"\"given full session, trains agent with policy gradient\"\"\"\n",
    "    _cumulative_rewards = get_cumulative_rewards(_rewards)\n",
    "    update.run({states: _states, actions: _actions,\n",
    "                cumulative_rewards: _cumulative_rewards})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \"\"\"play env with REINFORCE agent and train at the session end\"\"\"\n",
    "\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "\n",
    "        # action probabilities array aka pi(a|s)\n",
    "        action_probas = get_action_proba(s)\n",
    "\n",
    "        a = <pick random action using action_probas >\n",
    "\n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        # record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    train_step(states, actions, rewards)\n",
    "\n",
    "    return sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:27.590\n",
      "mean reward:70.340\n",
      "mean reward:129.570\n",
      "mean reward:188.330\n",
      "mean reward:211.530\n",
      "mean reward:240.490\n",
      "mean reward:235.760\n",
      "mean reward:218.030\n",
      "mean reward:258.470\n",
      "mean reward:184.760\n",
      "mean reward:298.920\n",
      "mean reward:507.360\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "s = tf.InteractiveSession()\n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    rewards = [generate_session() for _ in range(100)]  # generate new sessions\n",
    "\n",
    "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
    "\n",
    "    if np.mean(rewards) > 300:\n",
    "        print(\"You Win!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results & video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-08 03:29:10,315] Making new env: CartPole-v0\n",
      "[2017-04-08 03:29:10,324] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-04-08 03:29:10,329] Clearing 6 monitor files from previous run (because force=True was provided)\n",
      "[2017-04-08 03:29:10,336] Starting new video recorder writing to /home/jheuristic/Downloads/sonnet/sonnet/examples/videos/openaigym.video.0.14221.video000000.mp4\n",
      "[2017-04-08 03:29:16,834] Starting new video recorder writing to /home/jheuristic/Downloads/sonnet/sonnet/examples/videos/openaigym.video.0.14221.video000001.mp4\n",
      "[2017-04-08 03:29:23,689] Starting new video recorder writing to /home/jheuristic/Downloads/sonnet/sonnet/examples/videos/openaigym.video.0.14221.video000008.mp4\n",
      "[2017-04-08 03:29:33,407] Starting new video recorder writing to /home/jheuristic/Downloads/sonnet/sonnet/examples/videos/openaigym.video.0.14221.video000027.mp4\n",
      "[2017-04-08 03:29:45,840] Starting new video recorder writing to /home/jheuristic/Downloads/sonnet/sonnet/examples/videos/openaigym.video.0.14221.video000064.mp4\n",
      "[2017-04-08 03:29:56,812] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/jheuristic/Downloads/sonnet/sonnet/examples/videos')\n"
     ]
    }
   ],
   "source": [
    "# record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),\n",
    "                           directory=\"videos\", force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./videos/openaigym.video.0.14221.video000027.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# That's all, thank you for your attention!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
