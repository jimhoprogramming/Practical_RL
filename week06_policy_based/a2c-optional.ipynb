{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # in google colab uncomment this\n",
    "\n",
    "# import os\n",
    "\n",
    "# os.system('apt-get install -y xvfb')\n",
    "# os.system('wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall18/xvfb -O ../xvfb')\n",
    "# os.system('apt-get install -y python-opengl ffmpeg')\n",
    "# os.system('pip install pyglet==1.2.4')\n",
    "\n",
    "# os.system('python -m pip install -U pygame --user')\n",
    "\n",
    "# print('setup complete')\n",
    "\n",
    "# XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY = : 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Advantage-Actor Critic (A2C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will implement Advantage Actor Critic algorithm that trains on a batch of Atari 2600 environments running in parallel. \n",
    "\n",
    "Firstly, we will use environment wrappers implemented in file `atari_wrappers.py`. These wrappers preprocess observations (resize, grayscal, take max between frames, skip frames and stack them together) and rewards. Some of the wrappers help to reset the environment and pass `done` flag equal to `True` when agent dies.\n",
    "File `env_batch.py` includes implementation of `ParallelEnvBatch` class that allows to run multiple environments in parallel. To create an environment we can use `nature_dqn_env` function. Note that if you are using \n",
    "PyTorch and not using `tensorboardX` you will need to implement a wrapper that will log **raw** total rewards that the *unwrapped* environment returns and redefine the implemention of `nature_dqn_env` function here. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry_point is :gym.envs.atari:AtariEnv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jim/.local/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(require = False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry_point is :gym.envs.atari:AtariEnv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jim/.local/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(require = False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry_point is :gym.envs.atari:AtariEnv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jim/.local/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(require = False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry_point is :gym.envs.atari:AtariEnv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jim/.local/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(require = False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry_point is :gym.envs.atari:AtariEnv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jim/.local/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(require = False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry_point is :gym.envs.atari:AtariEnv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jim/.local/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(require = False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry_point is :gym.envs.atari:AtariEnv\n",
      "entry_point is :gym.envs.atari:AtariEnv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jim/.local/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(require = False)\n",
      "/home/jim/.local/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(require = False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<env_batch.SpaceBatch object at 0x7f640ca05630>\n",
      "[3 0 3 5 0 2 3 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from atari_wrappers import nature_dqn_env\n",
    "\n",
    "\n",
    "env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=8)\n",
    "obs = env.reset()\n",
    "assert obs.shape == (8, 84, 84, 4) # 8 个环境并行，84x84单色，4帧合并 \n",
    "assert obs.dtype == np.uint8\n",
    "print(env.action_space) # Discrete(6) 0-5\n",
    "print(env.action_space.sample()) # 8 个环境并行 ,每个连续按动同一个动作 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will need to implement a model that predicts logits and values. It is suggested that you use the same model as in [Nature DQN paper](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) with a modification that instead of having a single output layer, it will have two output layers taking as input the output of the last hidden layer. **Note** that this model is different from the model you used in homework where you implemented DQN. You can use your favorite deep learning framework here. We suggest that you use orthogonal initialization with parameter $\\sqrt{2}$ for kernels and initialize biases with zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_model_class(\n",
      "  (net): HybridSequential(\n",
      "    (0): Conv2D(None -> 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Conv2D(None -> 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): Conv2D(None -> 16, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (3): Conv2D(None -> 8, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (4): Conv2D(None -> 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): AvgPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False, global_pool=False, pool_type=avg, layout=NCHW)\n",
      "  )\n",
      "  (actions_conv2d): Conv2D(None -> 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (actions_desen): Dense(None -> 6, Activation(relu))\n",
      "  (values_conv2d): Conv2D(None -> 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (values_desen): Dense(None -> 1, Activation(sigmoid))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as torch\n",
    "# import torch as tf\n",
    "import mxnet as mx\n",
    "# <Define your model here>\n",
    "class my_model_class(mx.gluon.nn.HybridBlock):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(my_model_class, self).__init__(**kwargs)\n",
    "        self.net = mx.gluon.nn.HybridSequential()\n",
    "        self.net.add(mx.gluon.nn.Conv2D(channels=4,layout=\"NCHW\",kernel_size=3))\n",
    "        self.net.add(mx.gluon.nn.Conv2D(channels=16,layout=\"NCHW\",kernel_size=3))\n",
    "        self.net.add(mx.gluon.nn.Conv2D(channels=16,layout=\"NCHW\",kernel_size=2))\n",
    "        self.net.add(mx.gluon.nn.Conv2D(channels=8,layout=\"NCHW\",kernel_size=2))\n",
    "        self.net.add(mx.gluon.nn.Conv2D(channels=4,layout=\"NCHW\",kernel_size=3))\n",
    "        self.net.add(mx.gluon.nn.AvgPool2D(layout=\"NCHW\",pool_size=2))\n",
    "        #\n",
    "        self.actions_conv2d = mx.gluon.nn.Conv2D(channels=1,layout=\"NCHW\",kernel_size=3)\n",
    "        self.actions_desen = mx.gluon.nn.Dense(6,activation='relu')\n",
    "        #\n",
    "        self.values_conv2d = mx.gluon.nn.Conv2D(channels=1,layout=\"NCHW\",kernel_size=3)\n",
    "        self.values_desen = mx.gluon.nn.Dense(1,activation='sigmoid')\n",
    "    def hybrid_forward(self,F,x):\n",
    "        middle_x = self.net(x)\n",
    "        y_actions = self.actions_conv2d(middle_x)\n",
    "        y_actions = self.actions_desen(y_actions)\n",
    "        y_values = self.values_conv2d(middle_x)\n",
    "        y_values = self.values_desen(y_values)\n",
    "        return y_actions, y_values\n",
    "\n",
    "model = my_model_class()\n",
    "print(model)\n",
    "model.initialize(init=mx.init.Xavier())\n",
    "model.hybridize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need to define and use a policy that wraps the model. While the model computes logits for all actions, the policy will sample actions and also compute their log probabilities.  `policy.act` should return a dictionary of all the arrays that are needed to interact with an environment and train the model.\n",
    " Note that actions must be an `np.ndarray` while the other\n",
    "tensors need to have the type determined by your deep learning framework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntest_policy = Policy(model)\\ninputs = mx.nd.random.uniform(low=0, high=255,shape = (8,48,48,4),dtype='float32')\\nprint(inputs.shape)\\nrel = test_policy.act(inputs)\\n\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Policy():\n",
    "  def __init__(self, model):\n",
    "    self.model = model\n",
    "    \n",
    "  def act(self, inputs):\n",
    "    # Implement policy by calling model, sampling actions and computing their log probs>\n",
    "    # Should return a dict containing keys ['actions', 'logits', 'log_probs', 'values'].\n",
    "    # 生成以各种keys名称对应的m=envs行，actions = 1列，logits = 6列，log_probs = 6列，values = 1列的字典\n",
    "    \n",
    "    inputs = mx.nd.array(inputs,dtype = 'float32')\n",
    "    print(inputs.shape)\n",
    "    inputs = mx.nd.transpose(inputs, axes=(0,3,1,2))\n",
    "    print('转换后输入x维度：{}'.format(inputs.shape))\n",
    "    y_actions, y_values = self.model(inputs) # y_values 就是 V_hat_t(s_t,theata_v) \n",
    "    #print(y_actions.shape)\n",
    "    #print(y_values.shape)\n",
    "    rel = {}\n",
    "    logits = mx.nd.softmax(y_actions,axis=-1)\n",
    "    #print(logits.shape)\n",
    "    rel['logits'] = logits\n",
    "    m,n = logits.shape\n",
    "    rel['actions'] = []\n",
    "    for i in np.arange(m):\n",
    "        probs = logits[i,:].asnumpy()\n",
    "        #print(probs)\n",
    "        rel['actions'].append(np.random.choice(np.array([0,1,2,3,4,5]),1,p = probs)[0])\n",
    "    #print(rel['actions'])\n",
    "    log_probs = mx.nd.log_softmax(y_actions,axis=-1)\n",
    "    rel['log_probs'] = log_probs\n",
    "    rel['values'] = y_values\n",
    "    return rel\n",
    "'''\n",
    "test_policy = Policy(model)\n",
    "inputs = mx.nd.random.uniform(low=0, high=255,shape = (8,48,48,4),dtype='float32')\n",
    "print(inputs.shape)\n",
    "rel = test_policy.act(inputs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next will pass the environment and policy to a runner that collects partial trajectories from the environment. \n",
    "The class that does is is already implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from runners import EnvRunner\n",
    "# 这个运行程序与环境交互一定数量的步骤，并返回一个包含键的字典\n",
    "# 在每个键下面都有一个python列表，其中列出了与指定长度T（部分轨迹的大小）的环境的交互。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This runner interacts with the environment for a given number of steps and returns a dictionary containing\n",
    "keys \n",
    "\n",
    "* 'observations' \n",
    "* 'rewards' \n",
    "* 'resets'\n",
    "* 'actions'\n",
    "* all other keys that you defined in `Policy`\n",
    "\n",
    "under each of these keys there is a python `list` of interactions with the environment of specified length $T$ &mdash; the size of partial trajectory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the part of the model that predicts state values you will need to compute the value targets. \n",
    "Any callable could be passed to `EnvRunner` to be applied to each partial trajectory after it is collected. \n",
    "Thus, we can implement and use `ComputeValueTargets` callable. \n",
    "The formula for the value targets is simple:\n",
    "\n",
    "$$\n",
    "\\hat v(s_t) = \\sum_{t'=0}^{T - 1}\\gamma^{t'}r_{t+t'} + \\gamma^T \\hat{v}(s_{t+T}),\n",
    "$$\n",
    "\n",
    "In implementation, however, do not forget to use \n",
    "`trajectory['resets']` flags to check if you need to add the value targets at the next step when \n",
    "computing value targets for the current step. You can access `trajectory['state']['latest_observation']`\n",
    "to get last observations in partial trajectory &mdash; $s_{t+T}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 训练角色对于预测状态值的模型，您将需要计算值目标。任何可调用的都可以传递给EnvRunner，\n",
    "# 以便在收集后应用于每个部分轨迹。因此，我们可以实现和使用ComputeValueTargets callable。\n",
    "# 价值目标的公式很简单\n",
    "class ComputeValueTargets():\n",
    "  def __init__(self, policy, gamma=0.99):\n",
    "    self.policy = policy\n",
    "    self.gamma = gamma\n",
    "  def __call__(self, trajectory):\n",
    "    # This method should modify trajectory inplace by adding \n",
    "    # an item with key 'value_targets' to it. \n",
    "    # <Compute value targets for a given partial trajectory>\n",
    "    T = len(trajectory)\n",
    "    print('trajectory lenght :{}'.format(T))\n",
    "    t_array = mx.nd.array([i for i in range(T)])\n",
    "    gamma_array = mx.nd.ones(shape=(T,2)) * self.gamma\n",
    "    gamma_array[:,0] = gamma_array[:,0] ** t_array\n",
    "    gamma_array[:,1] = gamma_array[:,1] ** T\n",
    "    print('gamma_array:{}'.format(gamma_array))\n",
    "    print(\"trajectory['reward']:{}\".format(trajectory['rewards']))\n",
    "    print(\"trajectory['values']:{}\".format(trajectory['values']))\n",
    "    laster_v = [trajectory['values'][-1].asscalar() for i in range(T)]\n",
    "    print(laster_v)\n",
    "    r_v_array = mx.nd.array([trajectory['rewards'], laster_v]).T\n",
    "    print(r_v_array)\n",
    "    v_hat_s_t_array = mx.nd.sum(gamma_array * r_v_array, axis = 1)\n",
    "    print(v_hat_s_t_array)\n",
    "    rel = [mx.nd.sum(v_hat_s_t_array[i:]).asscalar() for i in np.arange(T)]\n",
    "    return rel    \n",
    "#test = ComputeValueTargets(policy = [1,2,3])\n",
    "#rel['rewards'] = [i for i in range(len(rel))]\n",
    "#print(test(rel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After computing value targets we will transform lists of interactions into tensors\n",
    "with the first dimension `batch_size` which is equal to `T * nenvs`, i.e. you essentially need\n",
    "to flatten the first two dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在计算了价值目标之后，我们将把交互列表转换成第一维度的张量，\n",
    "# 其大小等于T=5 * nenvs=8，也就是说，你基本上需要展平前两个维度。\n",
    "class MergeTimeBatch:\n",
    "  \"\"\" Merges first two axes typically representing time and env batch. \"\"\"\n",
    "  def __call__(self, trajectory):\n",
    "    # Modify trajectory inplace. \n",
    "    # <TODO: implement>\n",
    "    # T行一组，每个环境顺次连排\n",
    "    pass\n",
    "    return trajectory\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.defaultdict'>\n",
      "{'atcion': [1, 2, 3]}\n",
      "dict_items([('atcion', [1, 2, 3])])\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "a = defaultdict(list, {\"actions\": []})\n",
    "print(type(a))\n",
    "b = dict()\n",
    "b['atcion'] = [1,2,3]\n",
    "print(b)\n",
    "print(b.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 84, 84, 4)\n",
      "(8, 84, 84, 4)\n",
      "转换后输入x维度：(8, 4, 84, 84)\n",
      "(8, 84, 84, 4)\n",
      "转换后输入x维度：(8, 4, 84, 84)\n",
      "(8, 84, 84, 4)\n",
      "转换后输入x维度：(8, 4, 84, 84)\n",
      "(8, 84, 84, 4)\n",
      "转换后输入x维度：(8, 4, 84, 84)\n",
      "(8, 84, 84, 4)\n",
      "转换后输入x维度：(8, 4, 84, 84)\n",
      "trajectory lenght :8\n",
      "gamma_array:\n",
      "[[1.         0.92274475]\n",
      " [0.99       0.92274475]\n",
      " [0.98010004 0.92274475]\n",
      " [0.970299   0.92274475]\n",
      " [0.960596   0.92274475]\n",
      " [0.9509901  0.92274475]\n",
      " [0.9414802  0.92274475]\n",
      " [0.9320654  0.92274475]]\n",
      "<NDArray 8x2 @cpu(0)>\n",
      "trajectory['reward']:[array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.])]\n",
      "trajectory['values']:[\n",
      "[[0.00022474]\n",
      " [0.00022474]\n",
      " [0.00022474]\n",
      " [0.00022474]\n",
      " [0.00022474]\n",
      " [0.00022474]\n",
      " [0.00022474]\n",
      " [0.00022474]]\n",
      "<NDArray 8x1 @cpu(0)>, \n",
      "[[0.00011041]\n",
      " [0.00011041]\n",
      " [0.00011041]\n",
      " [0.00011041]\n",
      " [0.00011041]\n",
      " [0.00011041]\n",
      " [0.00011041]\n",
      " [0.00011041]]\n",
      "<NDArray 8x1 @cpu(0)>, \n",
      "[[0.00011381]\n",
      " [0.00011381]\n",
      " [0.00011381]\n",
      " [0.00011381]\n",
      " [0.00011381]\n",
      " [0.00011381]\n",
      " [0.00011381]\n",
      " [0.00011381]]\n",
      "<NDArray 8x1 @cpu(0)>, \n",
      "[[0.00095533]\n",
      " [0.00095533]\n",
      " [0.00095533]\n",
      " [0.00095533]\n",
      " [0.00095533]\n",
      " [0.00095533]\n",
      " [0.00095533]\n",
      " [0.00095533]]\n",
      "<NDArray 8x1 @cpu(0)>, \n",
      "[[0.00088089]\n",
      " [0.00088089]\n",
      " [0.00088089]\n",
      " [0.00088089]\n",
      " [0.00088089]\n",
      " [0.00088089]\n",
      " [0.00088089]\n",
      " [0.00088089]]\n",
      "<NDArray 8x1 @cpu(0)>]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The current array is not a scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-46667b51bff8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrunner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnsteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mComputeValueTargets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMergeTimeBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'latest_observation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Practical_RL/week06_policy_based/runners.py\u001b[0m in \u001b[0;36mget_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m       \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrajectory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-77-8d6e89861458>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, trajectory)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trajectory['reward']:{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rewards'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trajectory['values']:{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'values'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mlaster_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'values'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlaster_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mr_v_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rewards'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlaster_v\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-77-8d6e89861458>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trajectory['reward']:{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rewards'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trajectory['values']:{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'values'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mlaster_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'values'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlaster_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mr_v_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rewards'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlaster_v\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2581\u001b[0m         \"\"\"\n\u001b[1;32m   2582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2583\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The current array is not a scalar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2584\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2585\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The current array is not a scalar"
     ]
    }
   ],
   "source": [
    "model = model     # <Create your model here>\n",
    "policy_obj = Policy(model)\n",
    "runner = EnvRunner(env = env, policy = policy_obj, nsteps=5, transforms=[ComputeValueTargets(policy_obj),MergeTimeBatch()])\n",
    "print(runner.state['latest_observation'].shape)\n",
    "print(runner.get_next())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is the time to implement the advantage actor critic algorithm itself. You can look into your lecture,\n",
    "[Mnih et al. 2016](https://arxiv.org/abs/1602.01783) paper, and [lecture](https://www.youtube.com/watch?v=Tol_jw5hWnI&list=PLkFD6_40KJIxJMR-j5A1mkxK26gh_qg37&index=20) by Sergey Levine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C:\n",
    "  def __init__(self,\n",
    "               policy,\n",
    "               optimizer,\n",
    "               value_loss_coef=0.25,\n",
    "               entropy_coef=0.01,\n",
    "               max_grad_norm=0.5):\n",
    "    self.policy = policy\n",
    "    self.optimizer = optimizer\n",
    "    self.value_loss_coef = value_loss_coef\n",
    "    self.entropy_coef = entropy_coef\n",
    "    self.max_grad_norm = max_grad_norm\n",
    "    \n",
    "  def policy_loss(self, trajectory):\n",
    "    # You will need to compute advantages here. \n",
    "    <TODO: implement>\n",
    "    \n",
    "  def value_loss(self, trajectory):\n",
    "    <TODO: implement>\n",
    "    \n",
    "  def loss(self, trajectory):\n",
    "    <TODO: implement>\n",
    "      \n",
    "  def step(self, trajectory):\n",
    "    <TODO: implement>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can train your model. With reasonable hyperparameters training on a single GTX1080 for 10 million steps across all batched environments (which translates to about 5 hours of wall clock time)\n",
    "it should be possible to achieve *average raw reward over last 100 episodes* (the average is taken over 100 last \n",
    "episodes in each environment in the batch) of about 600. You should plot this quantity with respect to \n",
    "`runner.step_var` &mdash; the number of interactions with all environments. It is highly \n",
    "encouraged to also provide plots of the following quantities (these are useful for debugging as well):\n",
    "\n",
    "* [Coefficient of Determination](https://en.wikipedia.org/wiki/Coefficient_of_determination) between \n",
    "value targets and value predictions\n",
    "* Entropy of the policy $\\pi$\n",
    "* Value loss\n",
    "* Policy loss\n",
    "* Value targets\n",
    "* Value predictions\n",
    "* Gradient norm\n",
    "* Advantages\n",
    "* A2C loss\n",
    "\n",
    "For optimization we suggest you use RMSProp with learning rate starting from 7e-4 and linearly decayed to 0, smoothing constant (alpha in PyTorch and decay in TensorFlow) equal to 0.99 and epsilon equal to 1e-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2c = <Create instance of the algorithm> \n",
    "\n",
    "<Write your training loop>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
