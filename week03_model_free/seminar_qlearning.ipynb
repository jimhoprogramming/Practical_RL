{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning (3 points)\n",
    "\n",
    "This notebook will guide you through implementation of vanilla Q-learning algorithm.\n",
    "\n",
    "You need to implement QLearningAgent (follow instructions for each method) and use it on a number of tests below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In google collab, uncomment this:\n",
    "# !wget https://bit.ly/2FMJP5K -q -O setup.py\n",
    "# !bash setup.py 2>&1 1>stdout.log | tee stderr.log\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# If you are running locally, just ignore it\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY = : 1\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     12,
     17,
     18,
     20,
     22,
     25,
     35,
     64
    ]
   },
   "outputs": [],
   "source": [
    "#%%writefile qlearning.py\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, alpha, epsilon, discount, get_legal_actions):\n",
    "        \"\"\"\n",
    "        Q-Learning Agent\n",
    "        based on https://inst.eecs.berkeley.edu/~cs188/sp19/projects.html\n",
    "        Instance variables you have access to\n",
    "          - self.epsilon (exploration prob)\n",
    "          - self.alpha (learning rate)\n",
    "          - self.discount (discount rate aka gamma)\n",
    "\n",
    "        Functions you should use\n",
    "          - self.get_legal_actions(state) {state, hashable -> list of actions, each is hashable}\n",
    "            which returns legal actions for a state\n",
    "          - self.get_qvalue(state,action)\n",
    "            which returns Q(state,action)\n",
    "          - self.set_qvalue(state,action,value)\n",
    "            which sets Q(state,action) := value\n",
    "        !!!Important!!!\n",
    "        Note: please avoid using self._qValues directly. \n",
    "            There's a special self.get_qvalue/set_qvalue for that.\n",
    "        \"\"\"\n",
    "\n",
    "        self.get_legal_actions = get_legal_actions\n",
    "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "\n",
    "    def get_qvalue(self, state, action):\n",
    "        \"\"\" Returns Q(state,action) \"\"\"\n",
    "        return self._qvalues[state][action]\n",
    "\n",
    "    def set_qvalue(self, state, action, value):\n",
    "        \"\"\" Sets the Qvalue for [state,action] to the given value \"\"\"\n",
    "        self._qvalues[state][action] = value\n",
    "\n",
    "    #---------------------START OF YOUR CODE---------------------#\n",
    "\n",
    "    def get_value(self, state):\n",
    "        \"\"\"\n",
    "        Compute your agent's estimate of V(s) using current q-values\n",
    "        V(s) = max_over_action Q(state,action) over possible actions.\n",
    "        Note: please take into account that q-values can be negative.\n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return 0.0\n",
    "        if len(possible_actions) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        #<YOUR CODE HERE >\n",
    "        v_over_action = [self.get_qvalue(state,a) for a in possible_actions]\n",
    "        value = max(v_over_action) \n",
    "        return value\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        You should do your Q-Value update here:\n",
    "           Q(s,a) := (1 - alpha) * Q(s,a) + alpha * (r + gamma * V(s'))\n",
    "        \"\"\"\n",
    "\n",
    "        # agent parameters\n",
    "        gamma = self.discount\n",
    "        learning_rate = self.alpha\n",
    "\n",
    "        #<YOUR CODE HERE >\n",
    "        YOUR_QVALUE = (1 - learning_rate) * self.get_qvalue(state, action) + learning_rate * (reward + gamma * self.get_value(next_state))\n",
    "        self.set_qvalue(state, action, YOUR_QVALUE)\n",
    "\n",
    "    def get_best_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the best action to take in a state (using current q-values). \n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        #<YOUR CODE HERE >\n",
    "        qsa_all_a = {a:self.get_qvalue(state,a) for a in possible_actions}\n",
    "        index = np.argmax(qsa_all_a.values())\n",
    "        best_action = [a for a in qsa_all_a.keys()][index]\n",
    "        return best_action\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the action to take in the current state, including exploration.  \n",
    "        With probability self.epsilon, we should take a random action.\n",
    "            otherwise - the best policy action (self.get_best_action).\n",
    "\n",
    "        Note: To pick randomly from a list, use random.choice(list). \n",
    "              To pick True or False with a given probablity, generate uniform number in [0, 1]\n",
    "              and compare it with your probability\n",
    "        \"\"\"\n",
    "\n",
    "        # Pick Action\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "        action = None\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        # agent parameters:\n",
    "        epsilon = self.epsilon\n",
    "\n",
    "        #<YOUR CODE HERE >\n",
    "        if np.random.uniform(0,1)<epsilon:\n",
    "            chosen_action = np.random.choice(possible_actions,1)[0]\n",
    "        else:\n",
    "            chosen_action = self.get_best_action(state)\n",
    "        return chosen_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it on taxi\n",
    "\n",
    "Here we use the qlearning agent on taxi env from openai gym.\n",
    "You will need to insert a few agent functions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry_point is :gym.envs.toy_text.taxi:TaxiEnv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jim/.local/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(require = False)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from qlearning import QLearningAgent\n",
    "\n",
    "agent = QLearningAgent(alpha=0.5, epsilon=0.25, discount=0.99,\n",
    "                       get_legal_actions=lambda s: range(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_train(env, agent, t_max=10**4):\n",
    "    \"\"\"\n",
    "    This function should \n",
    "    - run a full game, actions given by agent's e-greedy policy\n",
    "    - train agent using agent.update(...) whenever it is possible\n",
    "    - return total reward\n",
    "    \"\"\"\n",
    "    total_reward = 0.0\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # get agent to pick action given state s.\n",
    "        # <YOUR CODE >\n",
    "        a = agent.get_action(s) \n",
    "        next_s, r, done, _ = env.step(a)\n",
    "\n",
    "        # train (update) agent for state s\n",
    "        # <YOUR CODE HERE >\n",
    "        agent.update(state = s, action = a, reward = r, next_state = next_s)\n",
    "        s = next_s\n",
    "        total_reward += r\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps = 2.9191091959171894e-05 mean reward = -200.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "rewards = []\n",
    "for i in range(1000):\n",
    "    rewards.append(play_and_train(env, agent))\n",
    "    agent.epsilon *= 0.99\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        clear_output(True)\n",
    "        print('eps =', agent.epsilon, 'mean reward =', np.mean(rewards[-10:]))\n",
    "        plt.plot(rewards)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Binarized state spaces\n",
    "\n",
    "Use agent to train efficiently on CartPole-v0.\n",
    "This environment has a continuous set of possible states, so you will have to group them into bins somehow.\n",
    "\n",
    "The simplest way is to use `round(x,n_digits)` (or numpy round) to round real number to a given amount of digits.\n",
    "\n",
    "The tricky part is to get the n_digits right for each state to train effectively.\n",
    "\n",
    "Note that you don't need to convert state to integers, but to __tuples__ of any kind of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry_point is :gym.envs.classic_control:CartPoleEnv\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "first state:[-0.0470169  -0.0179386  -0.01799376  0.04980367]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jim/.local/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(require = False)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'base' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-fded2e470610>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"first state:%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Error occured while running `from pyglet.gl import *`\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HINT: make sure you have OpenGL install. On Ubuntu, you can run 'apt-get install python-opengl'. If you're running on a server, you may need a virtual frame buffer; something like this should work: 'xvfb-run -s \\\"-screen 0 1400x900x24\\\" python <your_script.py>'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcarbon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCarbonConfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;31m# XXX remove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'base' is not defined"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"first state:%s\" % (env.reset()))\n",
    "plt.imshow(env.render('rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play a few games\n",
    "\n",
    "We need to estimate observation distributions. To do so, we'll play a few games and record all states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_states = []\n",
    "for _ in range(1000):\n",
    "    all_states.append(env.reset())\n",
    "    done = False\n",
    "    while not done:\n",
    "        s, r, done, _ = env.step(env.action_space.sample())\n",
    "        all_states.append(s)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "all_states = np.array(all_states)\n",
    "\n",
    "for obs_i in range(env.observation_space.shape[0]):\n",
    "    plt.hist(all_states[:, obs_i], bins=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gym.core import ObservationWrapper\n",
    "\n",
    "\n",
    "class Binarizer(ObservationWrapper):\n",
    "\n",
    "    def observation(self, state):\n",
    "\n",
    "        # state = <round state to some amount digits.>\n",
    "        # hint: you can do that with round(x,n_digits)\n",
    "        # you will need to pick a different n_digits for each dimension\n",
    "\n",
    "        return tuple(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = Binarizer(gym.make(\"CartPole-v0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_states = []\n",
    "for _ in range(1000):\n",
    "    all_states.append(env.reset())\n",
    "    done = False\n",
    "    while not done:\n",
    "        s, r, done, _ = env.step(env.action_space.sample())\n",
    "        all_states.append(s)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "all_states = np.array(all_states)\n",
    "\n",
    "for obs_i in range(env.observation_space.shape[0]):\n",
    "\n",
    "    plt.hist(all_states[:, obs_i], bins=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn binarized policy\n",
    "\n",
    "Now let's train a policy that uses binarized state space.\n",
    "\n",
    "__Tips:__ \n",
    "* If your binarization is too coarse, your agent may fail to find optimal policy. In that case, change binarization. \n",
    "* If your binarization is too fine-grained, your agent will take much longer than 1000 steps to converge. You can either increase number of iterations and decrease epsilon decay or change binarization.\n",
    "* Having 10^3 ~ 10^4 distinct states is recommended (`len(QLearningAgent._qvalues)`), but not required.\n",
    "* A reasonable agent should get to an average reward of >=50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent = QLearningAgent(alpha=0.5, epsilon=0.25, discount=0.99,\n",
    "                       get_legal_actions=lambda s: range(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rewards = []\n",
    "for i in range(1000):\n",
    "    rewards.append(play_and_train(env, agent))\n",
    "\n",
    "    # OPTIONAL YOUR CODE: adjust epsilon\n",
    "    if i % 100 == 0:\n",
    "        clear_output(True)\n",
    "        print('eps =', agent.epsilon, 'mean reward =', np.mean(rewards[-10:]))\n",
    "        plt.plot(rewards)\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
